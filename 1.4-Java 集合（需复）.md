原文链接：https://blog.csdn.net/a745233700/article/details/119709104Java 集合

# 0、说一下java集合

【从 `java` 集合框架--------->Collection和Map ------------>Collection的子接口 List、Set和Queue 特性和使用场景 ------------>子接口的实现类和子接口以及底层数据结构--------子接口的实现类-------->Map的使用场景和特性---------->子接口和实现类的底层数据结构--------子接口实现类的底层数据结构】

`java`集合，有两大接口，一个是存储单个元素的`Collection`接口，

​										另一个是存储`key-value`键值对的`Map`接口。

**Collection**接口主要有三个子接口 ：`List Set Queue`

- **List** :存储的元素是有序的、可重复的、有索引
  - **List**的主要实现类有 ：`ArrayList`， `LinkedList` 

- **Set**：存储的元素是无序的，不可重复的
  - **Set的主要实现类**有`hashSet`、`LinkedHashSet`  `TreeSet`

- **Queue** ： 按照特定的排队规则来确定顺序 存储的元素是有序的可重复的
  - **Queue**的主要实现类有`PriorityQueue`、`ArrayQueue` 

**Map**(用 key 来搜索的专家): 使用键值对（key-value）存储，类似于数学上的函数 y=f(x)，"x" 代表 key，"y" 代表 value，key 是无序的、不可重复的，value 是无序的、可重复的，每个键最多映射到一个值。

- Map的主要实现类有`HashMap` 、`LinkedHashMap`、`HashTable` 红黑树



# 0、讲一下ArrayList

**ArrayList**：实现List接口，是个动态数组，主要用来存储数据。

- 存储的元素是有序的、可重复的。如果存储基本类型的数据，如int，long，boolean，short，byte，那只存储它们对应的包装类。
- 底层的数据结构是object数组。当数组存满了的时候再往数组里面添加数据的时候会触发扩容机制进行扩容。

**特点** 

**增删慢：**每次删除元素，都需要更改数组长度、拷贝以及移动元素位置。

**查询快：**由于数组在内存中是一块连续空间，因此可以根据地址+索引的方式快速获取对应位置上的元素。

**线程不安全 ：**

- **【场景 **】在添加元素的时候会出现**数据覆盖的现象**、**数组下标越界**、**null值的情况** 
  - **数据覆盖/ null值 ：**多线程下 A线程执行add()方法，执行到elementData[size]=e;但没来得及去执行size++，这个时候线程B也执行elementData[size]=eb; 那么线程A添加的值被覆盖，线程A执行size+1；线程B 也执行size+1；只添加一个数据，但是size+2了，会出现null值
  - **数组下标越界 ：** 线程A去size++之前，线程B执行到 ensureCapacityInternal(size + 1) 此时线程A 执行size++，然后线程B执行elementData[size]=e 数组越界
  - **null值 ：**
- **【对应的解决方案 】**
  - 使用vector ：关于Vector如何实现线程安全的，而是在方法上加了锁，即synchronized 修饰方法
  - 使用

- ```java
  public boolean add(E e) {
      ensureCapacityInternal(size + 1);  // Increments modCount!!
      // 关键代码
      elementData[size++] = e;
      return true;
  }
  
  // elementData[size]=e;
  // size=size+1;
  // 这个操作不是原子性的 
  
  ```

  

# 1、ArrayList的底层原理

因为`ArrayList`的底层实现是数组，在`ArrayList`底层有一个叫做`elementData`的数组用来存储元素。当数组满了再添加元素的时候会触发扩容，

（1） 使用空参构造器 

- 使用空参构造器来创建ArrayList的时候，底层会创建一个空的数组，在第一次添加数据的时候才会给数组分配内存空间。默认容量是10，但是由于存在addAll()方法

  - 第一次添加数据的时候，会先判断数组所需的最小容量和默认值10 ，哪个更大，就分配多大的内存空间。

  - 当数据满了就会触发扩容，默认扩容后的容量是原来容量的1.5 倍【旧数组容量+旧数组容量>>1】

    会判断数组所需的最小容量 和默认的扩容后的容量哪个更大

  - 然后执行**`Arrays.copyOf(elementData, newCapacity);`**这个方法是真正实现扩容的，调用Arrays.copyOf()把原数组整个复制到新数组中

​      

2、ArrayList是线程安全的吗？ 为什么？ 怎么解决 ？

# 2、ArrayList和数组（Array）的区别

- （1）`ArrayList`是基于动态数组实现的，`Array`是基于静态数组实现的

  ​	`ArrayList`会根据实际存储的元素动态的扩容，而`Array`创建之后就不可以修改他的长度

  ​	并且创建的时候`ArrayList`不需要指定大小，而`Array`创建的时候必须指定大小

- （2）`ArrayList`只能存储对象（包括null），**存储基本数据类型的时候会通过自动装箱将其转化为对应的包装类**，而`Array`可以存储基本数据类型以及对象。

  （3）`ArrayList` **可以使用泛型 但是Array 不可以**。

  （4）`ArrayList`支持插入、删除、遍历等常见操作，

  `Array` 只是一个固定长度的数组，只能按照下标访问其中的元素，不具备动态添加、删除元素的能力。

  

# 3、**ArrayList 和LinkedList 区别

**底层数据结构 ---->是否保证线程安全-------->删除插入操作 ------->内存空间------->是否支持随机访问**

- `ArrayList` 的底层是`object`数组，而`LinkedList`的底层是**双向链表**。

-  `ArrayList`（实现了 `RandomAccess` 接口） 支持`ArrayList`是可以**【随机访问】**的，但是`LinkedList`不可以随机访问。

- `ArrayList` 和`LinkedList`都不能保证**线程安全**

- **插入和删除是否受元素位置的影响：**

- `ArrayList` 的空间浪费主要体现在在 list 列表的结尾会预留一定的容量空间，而 LinkedList 的空间花费则体现在它的每一个元素都需要消耗比 ArrayList 更多的空间（因为要存放直接后继和直接前驱以及数据）。

  ------

  

# 4、ArrayList的扩容机制（？

以无参构造器为例

**以无参数构造方法创建 `ArrayList` 时，实际上初始化赋值的是一个空数组。当真正对数组进行添加元素操作时，才真正分配容量。即向数组中添加第一个元素时，数组容量扩为 10。之后每次扩容 扩容之后的容量变为之前容量的1.5倍**

**过程分析 ：**

因为`ArrayList`的底层实现是数组，在`ArrayList`底层有一个叫做`elementData`的数组用来存储元素。

在使用无参构造器创建`ArrayList`的时候，实际上初始化赋值的时候是将一个叫做`DEFAULTCAPACITY_EMPTY_ELEMENTDATA`的空数组赋值给了`elementData`的数组，

在第一次使用`add(E)`的时候，才会为数组分配内存空间

- 首先会得到`minCapacity（10），然后minCapacity - elementData.length > 0 就会触发扩容，执行grow()`方法，在grow()方法中会计算扩容后的容量（`newCapacity = oldCapacity + (oldCapacity >> 1`);）然后判断扩容后的容量和最小容量哪个更大**，将更大的那个作为扩容后的容量，所以可以得出第一次扩容后的容量是10 。然后执行`Arrays.copyOf(elementData, newCapacity);`**这个方法是真正实现扩容的步骤。**

当添加到11个数据的时候`minCapacity - elementData.length > 0 `就会触发扩容，执行`grow()`方法



# 5、HashMap 和 HashTable 的区别

**0、线程安全------>效率------->null值------->底层数据结构------>扩容机制**

1、`HashMap`不是线程安全的，`HashTable`是线程安全的，`HashTable`是通过`Sychronized`关键字来保证线程安全的，所以`HashMap`的效率更高

2、`HashTable`底层的数据结构是数组+链表，但是`hashMap`在`jdk1.8`的时候底层的数据结构又添加 了一个红黑树

3、**初始容量大小和每次扩充容量大小的不同：** 

① 创建时如果不指定容量初始值，`Hashtable` 默认的初始大小为 11，之后每次扩充，容量变为原来的 2n+1。`HashMap` 默认的初始化大小为 16。之后每次扩充，容量变为原来的 2 倍。

② 创建时如果给定了容量初始值，那么 `Hashtable` 会直接使用你给定的大小，而 `HashMap` 会将其扩充为 2 的幂次方大小（`HashMap` 中的`tableSizeFor()`方法保证，下面给出了源代码）。也就是说 `HashMap` 总是使用 2 的幂作为哈希表的大小,后面会介绍到为什么是 2 的幂次方。

------



## 5.1  为什么`HashTable`线程安全却不常用？

`HashTable`为了保证线程安全是给每个关键方法都加上了`Sycronized`关键字，就相当于对`HashTable`对象加锁。

单线程下加锁和释放锁都需要消耗一定的资源。多线程下如果有一个线程在对`HashTable`对象操作，那么其他想对`HashTable`对象进行操作的线程就会处于阻塞状态。

# 6、说说`HashMap`

**`HashMap`是什么、特征、有什么用-------> 底层数据结构 ------->底层具体实现----->怎么扩容------>数据怎么拷贝**

## 6.1 `HashMap`是什么？ 有什么用？

- `HashMap` 是Map的实现类，是存储`key-value`键值对的集合，并且key值是唯一的。存入的数据是无序的。

- **1.8之前 底层用的是数组（散列表）和链表来存储数据，1.8及其之后 用的是数组链表和红黑树来存储数据,**

  用哈希算法来计算数据要存储位置，**使用链表和红黑树来解决哈希碰撞**。

  【链表、红黑树的存在是为了解决哈希冲突，红黑树的存在是为了解决链表过长时，查询操作的性能可能会下降的问题。**当数组的长度大于64并且链表的长度大于8，此时这个链表会变成红黑树**】

- `hashMap`是线程不安全的

## 6.2底层数据结构

1.8之前 底层用的是数组（散列表）和链表来存储数据

1.8及其之后 用的是数组链表和红黑树来存储数据【**当数组的长度大于64并且链表的长度大于8，此时这个链表会变成红黑树**】

- 链表、红黑树的存在是为了解决哈希冲突，红黑树的存在是为了解决链表过长时，查询操作的性能可能会下降的问题

## 6.3底层怎么实现插入数据

1.8 之前用的是数组和链表来存储数据，1.8及其之后用的是数组链表和红黑树来存储数据，用哈希算法来计算数据要存储位置，使用链表和红黑树来解决**哈希碰撞**。

**【哈希算法】**

**（1）** 首先计算key的`hashcode`值再将`hashCode值`经过扰动函数处理得到**hash值**

**！**【**为什么要经过扰动函数？不同对象计算出的`hashCode`值是可能相同的，增加扰动函数是为了添加随机性，使得散列更加均匀**】

**（2）**然后将计算出来的`hash`值和`n-1` 进行**&运算**，这里得出的值就是**数组的下标**,数组下标就是元素要存放的位置

**（3）**然后去数组中找到对应的位置，如果数组上该位置是空的，那就将`key`和`value`封装为`Entry/Node`存储在这个位置上。如果当前位置存在元素的话，就判断**该元素与要存入的元素的 hash 值**（相同就发生了hash碰撞）以及 key 是否相同，如果相同的话，直接覆盖， key不相同就通过拉链法解决冲突。<u>**1.8之前用的是头插法，以后用的就是尾插法。**</u>

判断发生冲突之后会遍历链表和红黑树，就判断目前位置上 的元素与要存入的元素的 hash 值以及 key 是否相同

### 6.3.1 为什么`hashMap`的数组长度总为2的幂次方

- (1)计算数组下标的时候使用的是`hash&(n-1)` ，**在n为2的次幂的时候 hash对n取余和hash&(n-1) 得到的结果是一致的**，**采用二进制位操作 &，相对于%能够提高运算效率**。

### 6.3.2 为什么`hashMap`的数组扩容的时候长度都必须为原来的2倍

- 数组扩容数据迁移的时候不需要像`JDK1.7`的实现那样重新计算`hash`，只需要看看原来的`hash`值新增的那个`bit`是1还是0就好了，是0的话索引没变，是1的话索引变成原位置+原数组长度。计算节点在`table`中的下标的方法是：`hash&(oldTable.length-1)`，扩容之后，`table`长度翻倍，计算`table`下标的方法是`hash&(newTable.length-1)`，也就是`hash&(oldTable.length*2-1)`，于是我们有了这样的结论：这新旧两次计算下标的结果，要不然就相同，要不然就是新下标等于旧下标加上旧数组的长度。

  



## 6.4 怎么扩容

　那么HashMap什么时候进行扩容呢？当hashmap中的元素个数超过数组大小*loadFactor时，就会进行数组扩容

【扩容 resize方法】

如果空参的构造器时

1.7 ：使用空参构造器的时候，底层创建一个初始容量为16的数组

1.8 ：使用空参构造器的时候，底层没有创建数组，等到第一次使用put()添加数据的时候才创建了容量为16的数组。

- 默认的**装载因子为0.75**，用装载因子数组的长度计算出触发扩容的阈值。当HashMap中元素的个数达到这个阈值的时候就触发扩容。

- 扩容的时候容量扩大到原来的2倍。

如果使用带有初始容量的构造器时，底层会将初始容量扩充为2的幂次方的容量【如果是3，则初始容量为4，如果是18 则初始容量为 32】，根据装载因子计算触发扩容的阈值，之后的每次容量都是原来的2倍数

### 6.4.1数据怎么拷贝

**在扩充HashMap的时候，不需要像JDK1.7的实现那样重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成原位置+原数组长度。**

`hashMap`扩容时数据拷贝的时候不需要重新计算hash值

在`JDK1.8`中，对`HashMap`进行了优化。

    经过rehash之后元素的位置，要么是在原位置，要么是在原位置再移动2次幂的位置。HashMap的数组长度恒定为2的n次方，也就是说只会为16，32，64，128这种数。即便你给的初始值是13，最后数组长度也会变成16，它会取与你传入的数最近的一个2的n次方的数。
> 正常情况下，计算节点在table中的下标的方法是：`hash&(oldTable.length-1)`，扩容之后，table长度翻倍，计算`table`下标的方法是`hash&(newTable.length-1)`，也就是`hash&(oldTable.length*2-1)`，于是我们有了这样的结论：这新旧两次计算下标的结果，要不然就相同，要不然就是新下标等于旧下标加上旧数组的长度。
>
> 原文链接：https://blog.csdn.net/lkforce/article/details/89521318

如果b所在的那一位是0，那么新table按位与的结果和旧table的结果就相同，反之如果b所在的那一位是1，则新`table`按位与的结果就比旧table的结果多了10000（二进制），而这个二进制10000就是`旧table`的长度16。



# 7.HashMap的底层数据结构，为什么要使用红黑树？

为什么使用红黑树 ？ 就隐含着为什么不使用别的数据结构

- **红黑树的存在是为了解决链表过长导致在搜索、插入和删除操作时的性能下降这个问题的。**（解决哈希冲突）

  当链表长过长时会转换成红黑树，那能不能使用AVL树替代呢？


## 7.1 当链表长过长时会转换成红黑树，那能 不能使用AVL树替代呢？

AVL树是完全平衡二叉树，要求每个结点的左右子树的高度之差的绝对值最多为1，而红黑树通过适当的放低该条件（红黑树限制从根到叶子的最长的可能路径不多于最短的可能路径的两倍长，结果是这个树大致上是平衡的），以此来减少插入/删除时的平衡调整耗时，从而获取更好的性能，虽然会导致红黑树的查询会比AVL稍慢，但相比插入/删除时获取的时间，这个付出在大多数情况下显然是值得的

## 7.2 为什么HashMap的容量是2的倍数呢？

- (1)要计算出元素存储的位置需要先对数组的长度进行取余运算

- **取余(%)操作中如果除数是 2 的幂次则等价于与其除数减一的与(&)操作，并且 **采用二进制位操作 &，相对于%能够提高运算效率。

## 7.3 为什么HashMap链表转红黑树的阈值为8呢？

【太高太低都不好，装载因子是0.75】

- 是时间和性能的

# 8.HashMap put的过程（1.7 和1.8的区别）

###### 1.7 使用put()的过程

**对 putVal 方法添加元素**

（1）首先会判断底层数组是否已经创建，如果没有创建的话就会先创建一个初始容量为16的数组,并且计算出触发扩容的阈值

（1）将计算出来的key的hashcode值经过扰动函数处理得到的值和（数组长度-1）进行&运算，得到用来散列的hash值，就是元素要存放的位置

（2）先判断是否需要扩容，【size和阈值的大小比较】扩容为原来的2倍，更新触发扩容的阈值  ，然后重新计算节点的hash值加到新的数组当中。

（3）来判断数组中对应的位置是否为空，为空的话将key-value封装为Entry存储到里面，如果不为空，则判断当前节点的hash和key值是否相同，如果相同的话就直接覆盖，否则则意味着发生了冲突，依次遍历链表，判断时候有hash、key值都相同，有的话直接替换value值，没有的话最后用头插法将数据插入到链表中

###### 1.8 put的过程

- **jdk1.8** 

（1）调用put方法的时候首先会计算出来key的hashCode值，底层是调用putVal()方法

**对 putVal 方法添加元素**

（2）然后会判断底层用来存储数据的数组是否为null或者长度为0，如果是的话会扩容，创建一个长度为16的数组

（3）将hashCode值进行扰动函数处理后和数组长度-1 进行&运算，得到用来散列的hash值

（4）判断数组中对应的位置是否为空，如果为空的话，就将key-value-hash-nextNode 封装为Node 存储到数组中该位置，然后判断是否需要扩容

（5）如果不为空，则需要判断hash值、key值和当前位置上的hash值、key值是否相同，如果相同的话就覆盖，如果不相同则说明发生了冲突。（用拉链法解决冲突，并且用尾插法插到链表当中）

（6）判断当前位置上的节点是链表还是红黑树

【如果是红黑树，那就直接遍历，判断节点是否相同，相同覆盖，不相同就直接插入树中。

】

【如果是链表，遍历节点，判断hash值、key值和当前位置上的hash值、key值是否相同，如果是的话则覆盖当前位置上的节点，不相同一直遍历到链表尾部，用尾插法插入

然后就判断是否需要扩容，如果数组长度大于64，并且链表长度>8则将当前链表转为红黑树

如果链表长度大于8但是数组长度<64,则数组扩容，长度为原来数组长度的2倍】

然后再来判断数组是否需要扩容

###### 为什么要使用红黑树不使用B树？

综上所述，HashMap选择使用红黑树而不是B树，主要是为了在时间复杂度和空间复杂度上取得一个平衡，并且保持相对简单的实现。

###### 红黑树和链表之间的转化？

当数组中某个位置的节点达到8个时，会触发 treeifyBin() 方法将链表节点（Node）转红黑树节点（TreeNode，间接继承Node），转成红黑树节点后，其实链表的结构还存在，通过next属性维持，红黑树节点在进行操作时都会维护链表的结构，并不是转为红黑树节点后，链表结构就不存在了。当数组中某个位置的节点在移除后达到6个时，并且该索引位置的节点为红黑树节点，会触发 untreeify() 将红黑树节点转化成链表节点。


# 9 HashMap不是线程安全，多线程下会出现什么问题？

**在jdk 1.7 中使用HashMap会出现的问题 ：由于头插法会使得在多线程环境下多个线程同时扩容造成死循环、以及put操作造成的覆盖两个线程 1,2 同时进行 put 操作，并且发生了哈希冲突。**

- 两个线程 1,2 同时进行 put 操作，并且发生了哈希冲突（hash 函数计算出的插入下标是相同的）。

  不同的线程可能在不同的时间片获得 CPU 执行的机会，当前线程 1 执行完哈希冲突判断后，由于时间片耗尽挂起。线程 2 先完成了插入操作。

  随后，线程 1 获得时间片，由于之前已经进行过 hash 碰撞的判断，所有此时会直接进行插入，这就导致线程 2 插入的数据被线程 1 覆盖了。

  - ，JDK1.8 版本的 HashMap 采用了尾插法而不是头插法来避免链表倒置，使得插入的节点永远都是放在链表的末尾，避免了链表中的环形结构。但是还是不建议在多线程下使用 `HashMap`，因为多线程下使用 `HashMap` 还是会存在数据覆盖的问题。

    ------

    

通过一个 modCount 值记录修改的次数，对 HashMap 的修改操作都会增加这个值。迭代器在初始过程中会将这个值赋给 exceptedModCount ，在迭代的过程中，如果发现 modCount 和 exceptedModCount 的值不一致，代表有其他线程修改了Map，就立刻抛出异常。


### 9.1什么办法能解决HashMap线程不安全的问题呢

加锁、或者使用ConcurrentHashMap、或者使用HashTable





# 10  HashMap 在 JDK7 和 JDK8 有哪些区别?

① 数据结构：在 JDK7 及之前的版本，HashMap 的数据结构可以看成“数组+链表”，在 JDK8 及之后的版本，数据结构可以看成"数组+链表+红黑树"，当**链表的长度超过8并且数组长度>64**时，链表就会转换成红黑树，从而降低时间复杂度（由O(n) 变成了 O(logN)），提高了效率。

【jdk7是直接创建数组，而jdk8是将负载因子赋给当前实例，没有创建数组，在第一次调用put时才会创建数组。】

② 对数据重哈希：JDK8 及之后的版本，对 hash() 方法进行了优化，重新计算 hash 值时，让 hashCode 的高16位参与异或运算，目的是在 table 的 length较小的时候，在进行计算元素存储位置时，也让高位也参与运算。

③ 在 JDK7 及之前的版本，在添加元素的时候，采用头插法，所以在扩容的时候，会导致之前元素相对位置倒置了，在多线程环境下扩容可能造成环形链表而导致死循环的问题。DK1.8之后使用的是尾插法，扩容是不会改变元素的相对位置

④ 扩容时重新计算元素的存储位置的方式：JDK7 及之前的版本重新计算存储位置是直接使用 hash & (table.length-1)；JDK8 使用节点的hash值与旧数组长度进行位与运算，如果运算结果为0，表示元素在新数组中的位置不变；否则，则在新数组中的位置下标=原位置+原数组长度。

⑤ **JDK7 是先扩容后插入**，这就导致无论这次插入是否发生hash冲突都需要进行扩容，但如果这次插入并没有发生Hash冲突的话，那么就会造成一次无效扩容；JDK8是先插入再扩容的，优点是减少这一次无效的扩容，原因就是如果这次插入没有发生Hash冲突的话，那么其实就不会造成扩容.【？？？？？？？？？？】
————————————————

[Java基础问题(part2) HashMap夺命连环问 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/538672799)

![img](https://img-blog.csdnimg.cn/img_convert/5a0d36e41297dfe9011e6d1d22a227e3.png)

![img](https://img-blog.csdnimg.cn/img_convert/3e0ef497f95c88e6d409cd6f0737bfe3.png)

![点击查看图片来源](https://img-blog.csdnimg.cn/img_convert/ab8a2704591e94591cd7baf150fce99a.png)

![点击查看图片来源](https://img-blog.csdnimg.cn/20201206181718431.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW56aG9uZ2hhb3Fpbmc=,size_16,color_FFFFFF,t_70)



![img](https://img-blog.csdnimg.cn/img_convert/9ea4cf965361c4f6b744d884b9d2ef37.png)



## 2、ConcurrentHashMap 线程安全实现原理是什么？

### JDK 1.7 和 JDK 1.8 的 ConcurrentHashMap 实现有什么不同？

- **线程安全实现方式**：JDK 1.7 采用 `Segment` 分段锁来保证安全， `Segment` 是继承自 `ReentrantLock`。JDK1.8 放弃了 `Segment` 分段锁的设计，采用 `Node + CAS + synchronized` 保证线程安全，锁粒度更细，`synchronized` 只锁定当前链表或红黑二叉树的首节点。
- **Hash 碰撞解决方法** : JDK 1.7 采用拉链法，JDK1.8 采用拉链法结合红黑树（链表长度超过一定阈值时，将链表转换为红黑树）。
- **并发度**：JDK 1.7 最大并发度是 Segment 的个数，默认是 16。JDK 1.8 最大并发度是 Node 数组的大小，并发度更大。

------



### 2.1ConcurrentHashMap 如何实现线程安全？

【底层数据结构以及是如何实现并发的？】

- 1.7 ： **底层的数据结构是 segement 数组+ 链表** 。**一个segment中包含了多个hashEntry** （相当于对哈希桶进行分块，一个segment 对应一块）**，segement实现了ReentrantLock ，充当了锁的角色**，相当于一次对多个hashEntry进行加锁、那么对操作不在这个segement 下的hashEntry的线程不会有影响，并且使用了**volatile修饰共享变量【底层用来存储数据的table数组】实现共享变量在多线程下的可见性**，实现了并发。
- 1.8 ： **底层的数据结构是Node数组+链表+红黑树**，**并且使用cas+syschronized关键字对链表头节点或者红黑树的根节点加锁从而实现并发，并且使用volatile修饰共享资源【【底层用来存储数据的table数组】】实现共享资源的可见性。**



> 1、ConcurrentHashMap在JDK 1.7中使用的数组 加 链表的结构，其中数组分为两类，大树组Segment 和 小数组 HashEntry，而加锁是通过给Segment添加ReentrantLock重入锁来保证线程安全的。
>
> 2、ConcurrentHashMap在JDK1.8中使用的是数组 加 链表 加 红黑树的方式实现，就相当于是**ConcurrentHashMap通过对头结点加锁来保证线程安全的** 它是通过 CAS 或者 synchronized  来保证线程安全的，并且缩小了锁的粒度，查询性能也更高。

### **（1） 1.8 版本 初始化的安全性**

**使用乐观锁CAS 和双重锁检验来保证初始化的线程安全** 

```java
private transient volatile int sizeCtl;
```

***ConcurrentHashMap的实现中有一个叫做sizeCtl的参数 ，并且使用volatile修饰保证对其他线程的可见性。它是一个标识：***

- 这个值等于0 的时候意味着这个容器刚刚创建。
- 等于-1 表示数组table正在初始化、<-1表示正在扩容  ，
- 大于0 就表示下次扩容时的阈值（如果以没有初始化就表示容器的容量）

 Node[] 数组在第一次调用put的时候才会初始化：

```java
private final Node<K,V>[] initTable() {
    Node<K,V>[] tab; int sc;
    while ((tab = table) == null || tab.length == 0) {
        if ((sc = sizeCtl) < 0)
//调用 Thread.yield() 方法会使当前正在执行的线程让出CPU，将CPU资源让给其他具有相同或更高优先级的线程。
            //但是不处于阻塞的状态
            Thread.yield(); // lost initialization race; just spin
        else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) {
            try {
                if ((tab = table) == null || tab.length == 0) {
                    int n = (sc > 0) ? sc : DEFAULT_CAPACITY;
                    @SuppressWarnings("unchecked")
                    Node<K,V>[] nt = (Node<K,V>[])new Node<?,?>[n];
                    table = tab = nt;
                    sc = n - (n >>> 2);
                }
            } finally {
                sizeCtl = sc;
            }
            break;
        }
    }
    return tab;
}
```

在这个初始化方法中

- 1、判断数组是否为空并且数组长度是否为0 

  - 否 已经有其他线程完成初始化了，直接访问

  - 是 要进行初始化，执行initTable()方法

- 2、在initTable()方法中通过cas操作和dcl来保证线程安全

  初始化操作之前要判断sizeCtl的值是否<0

  - 是 说明已经有线程在初始化了，当前线程执行yield()
  - 否 说明没有线程进行初始化，由当前线程来进行初始化

- 3、执行**compareAndSwapInt(this, SIZECTL, sc, -1) 将sizeCtl的值改为-1 这个操作只有一个线程能够成功，其他线程都会失败**
  
  - 通过cas操作来将sizeCtl的值改为-1
- 4、再次判断数组是否为空并且数组长度是否为0 
  - 否 直接return 
  - 是 初始化数组，创建一个容量为16的数组

![image-20230725154620685](C:\Users\16055\AppData\Roaming\Typora\typora-user-images\image-20230725154620685.png)

```java

```

### **（2）扩容的时候** 

- 什么时候会触发扩容

  - 1、使用put()的时候

    1.1 链表长度大于8 并且数组长度小于64 则数组扩容

    1.2  容器中的元素的个数大于 阈值

  - 2、使用putAll()

- 正在扩容的时候

  ```java
  //第一条扩容线程设置的某个特定基数
  U.compareAndSwapInt(this, SIZECTL, sc, (rs << RESIZE_STAMP_SHIFT) + 2)
  //后续线程加入扩容大军时每次加 1
  U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)
  //线程扩容完毕退出扩容操作时每次减 1
  U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)
  ```

- ```java
  //  addCount(1L, binCount); 只要执行了插入操作 bincout就会大于0
  private final void addCount(long x, int check) {
   // 省去计算容器元素个数的代码     
      if (check >= 0) {
          Node<K,V>[] tab, nt; int n, sc;
          // s是计算出来的容器中的数据的总数
         //判断是否需要扩容
          while (s >= (long)(sc = sizeCtl) && (tab = table) != null &&
                 (n = tab.length) < MAXIMUM_CAPACITY) {
              // 计算一个标识戳
              int rs = resizeStamp(n);
              //是否已经有线程进行扩容了
              if (sc < 0) {
                 //判断扩容是否结束或者并发扩容线程数是否已达最大值，如果是的话直接结束while循环
                  if ((sc >>> RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 ||
                      sc == rs + MAX_RESIZERS || (nt = nextTable) == null ||
                      transferIndex <= 0)
                      break;
                    //扩容还未结束，并且允许扩容线程加入，此时加入扩容大军中
                  if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1))
                      transfer(tab, nt);
              }
            //如果集合还未处于扩容状态中，则进入扩容方法，并首先初始化 nextTab 数组，也就是新数组
         //(rs << RESIZE_STAMP_SHIFT) + 2 为首个扩容线程所设置的特定值，后面扩容时会根据线程是否为这个值来确定是否为最后一个线程
              else if (U.compareAndSwapInt(this, SIZECTL, sc,
                                           (rs << RESIZE_STAMP_SHIFT) + 2))
                  transfer(tab, null);
              s = sumCount();
          }
      }
  }
  ```

###### 扩容的步骤

- 1、计算每条线程处理桶的个数【主要是进行数据迁移操作】，每条线程处理桶的个数至少为16个

  如果cpu为单核，则由一个线程进行处理，如果计算出来的结果少于16，则一条线程处理16个桶。

  stride来记录线程处理的桶的个数，并且线程是从桶的右边到左边进行数据迁移的，有两个参数来记录左右边界。

  ```
   if ((stride = (NCPU > 1) ? (n >>> 3) / NCPU : n) < MIN_TRANSFER_STRIDE)
          stride = MIN_TRANSFER_STRIDE; 
  ```

  

- 2、 新建一个newTable 并且新建一个占位对象

  该占位对象的 hash 值为 -1 该占位对象存在时表示集合正在扩容状态，key、value、next 属性均为 null ，nextTable 属性指向扩容后的数组
     该占位对象主要有两个用途：
     1、占位作用，用于标识数组该位置的桶已经迁移完毕，处于扩容中的状态。
     2、作为一个转发的作用，扩容期间如果遇到查询操作，遇到转发节点，会把该查询操作转发到新的

- 3、线程从右到左将桶里面的数据进行迁移 

  迁移的逻辑如下：

  - 并且将上面创建的fwd节点赋值给旧的容器的数组。


```java
private final void transfer(Node<K,V>[] tab, Node<K,V>[] nextTab) {
    int n = tab.length, stride;
   
    if ((stride = (NCPU > 1) ? (n >>> 3) / NCPU : n) < MIN_TRANSFER_STRIDE)
        stride = MIN_TRANSFER_STRIDE; // subdivide range
    //stride为每条线程应该处理的桶的个数
    if (nextTab == null) {            // initiating
        try {
            @SuppressWarnings("unchecked")
            Node<K,V>[] nt = (Node<K,V>[])new Node<?,?>[n << 1];//// 初始化新数组(原数组长度的2倍)
            nextTab = nt;
        } catch (Throwable ex) {      // try to cope with OOME
            sizeCtl = Integer.MAX_VALUE;
            return;
        }
        nextTable = nextTab;
        transferIndex = n;
    }
    int nextn = nextTab.length;
       //新建一个占位对象，该占位对象的 hash 值为 -1 该占位对象存在时表示集合正在扩容状态，key、value、next 属性均为 null ，nextTable 属性指向扩容后的数组
    //该占位对象主要有两个用途：
    //   1、占位作用，用于标识数组该位置的桶已经迁移完毕，处于扩容中的状态。
    //   2、作为一个转发的作用，扩容期间如果遇到查询操作，遇到转发节点，会把该查询操作转发到新的数组上去，不会阻塞查询操作。
 
    ForwardingNode<K,V> fwd = new ForwardingNode<K,V>(nextTab);
  //该标识用于控制是否继续处理下一个桶，为 true 则表示已经处理完当前桶，可以继续迁移下一个桶的数据
    boolean advance = true;
  //该标识用于控制扩容何时结束，该标识还有一个用途是最后一个扩容线程会负责重新检查一遍数组查看是否有遗漏的桶
    boolean finishing = false; // to ensure sweep before committing nextTab
    for (int i = 0, bound = 0;;) {
        Node<K,V> f; int fh;
        while (advance) {
            int nextIndex, nextBound;
            if (--i >= bound || finishing)
                advance = false;
            else if ((nextIndex = transferIndex) <= 0) {
                i = -1;
                advance = false;
            }
            else if (U.compareAndSwapInt
                     (this, TRANSFERINDEX, nextIndex,
                      nextBound = (nextIndex > stride ?
                                   nextIndex - stride : 0))) {
                bound = nextBound;
                i = nextIndex - 1;
                advance = false;
            }
        }
        if (i < 0 || i >= n || i + n >= nextn) {
            int sc;
            if (finishing) {
                nextTable = null;
                table = nextTab;
                sizeCtl = (n << 1) - (n >>> 1);
                return;
            }
            if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) {
                if ((sc - 2) != resizeStamp(n) << RESIZE_STAMP_SHIFT)
                    return;
                finishing = advance = true;
                i = n; // recheck before commit
            }
        }
        else if ((f = tabAt(tab, i)) == null)
            advance = casTabAt(tab, i, null, fwd);
        else if ((fh = f.hash) == MOVED)
            advance = true; // already processed
        else {
            synchronized (f) {
                if (tabAt(tab, i) == f) {
                    Node<K,V> ln, hn;
                    if (fh >= 0) {
                        int runBit = fh & n;
                        Node<K,V> lastRun = f;
                        for (Node<K,V> p = f.next; p != null; p = p.next) {
                            int b = p.hash & n;
                            if (b != runBit) {
                                runBit = b;
                                lastRun = p;
                            }
                        }
                        if (runBit == 0) {
                            ln = lastRun;
                            hn = null;
                        }
                        else {
                            hn = lastRun;
                            ln = null;
                        }
                        for (Node<K,V> p = f; p != lastRun; p = p.next) {
                            int ph = p.hash; K pk = p.key; V pv = p.val;
                            if ((ph & n) == 0)
                                ln = new Node<K,V>(ph, pk, pv, ln);
                            else
                                hn = new Node<K,V>(ph, pk, pv, hn);
                        }
                        setTabAt(nextTab, i, ln);
                        setTabAt(nextTab, i + n, hn);
                        setTabAt(tab, i, fwd);
                        advance = true;
                    }
                    else if (f instanceof TreeBin) {
                        TreeBin<K,V> t = (TreeBin<K,V>)f;
                        TreeNode<K,V> lo = null, loTail = null;
                        TreeNode<K,V> hi = null, hiTail = null;
                        int lc = 0, hc = 0;
                        for (Node<K,V> e = t.first; e != null; e = e.next) {
                            int h = e.hash;
                            TreeNode<K,V> p = new TreeNode<K,V>
                                (h, e.key, e.val, null, null);
                            if ((h & n) == 0) {
                                if ((p.prev = loTail) == null)
                                    lo = p;
                                else
                                    loTail.next = p;
                                loTail = p;
                                ++lc;
                            }
                            else {
                                if ((p.prev = hiTail) == null)
                                    hi = p;
                                else
                                    hiTail.next = p;
                                hiTail = p;
                                ++hc;
                            }
                        }
                        ln = (lc <= UNTREEIFY_THRESHOLD) ? untreeify(lo) :
                            (hc != 0) ? new TreeBin<K,V>(lo) : t;
                        hn = (hc <= UNTREEIFY_THRESHOLD) ? untreeify(hi) :
                            (lc != 0) ? new TreeBin<K,V>(hi) : t;
                        setTabAt(nextTab, i, ln);
                        setTabAt(nextTab, i + n, hn);
                        setTabAt(tab, i, fwd);
                        advance = true;
                    }
                }
            }
        }
    }
}
```



volatile +cas + sychronized

> 减小锁粒度：将 Node 链表的头节点作为锁，若在默认大小 16 情况下，将有 16 把锁，大大减小了锁竞争（上下文切换），就像开头所说，将串行的部分最大化缩小，在理想情况下线程的 put 操作都为并行操作。同时直接锁住头节点，保证了线程安全
> 使用了 volatile 修饰 table 变量，并使用 Unsafe 的 getObjectVolatile() 方法拿到最新的 Node
> CAS 操作：如果上述拿到的最新的 Node 为 null，则说明还没有任何线程在此 Node 位置进行插入操作，说明本次操作是第一次
> synchronized 同步锁：如果此时拿到的最新的 Node 不为 null，则说明已经有线程在此 Node 位置进行了插入操作，此时就产生了 hash 冲突；此时的 synchronized 同步锁就起到了关键作用，防止在多线程的情况下发生数据覆盖（线程不安全），接着在 synchronized 同步锁的管理下按照相应的规则执行操作

    当 hash 值相同并 key 值也相同时，则替换掉原 value
    否则，将数据插入链表或红黑树相应的节点
————————————————
原文链接：https://blog.csdn.net/weixin_38192427/article/details/122537790

### （3）put操作的时候

```java
final V putVal(K key, V value, boolean onlyIfAbsent) {
    if (key == null || value == null) throw new NullPointerException();
    int hash = spread(key.hashCode());
    int binCount = 0;
    for (Node<K,V>[] tab = table;;) {
        Node<K,V> f; int n, i, fh;
        if (tab == null || (n = tab.length) == 0)
            tab = initTable();
        else if ((f = tabAt(tab, i = (n - 1) & hash)) == null) {
            if (casTabAt(tab, i, null,
                         new Node<K,V>(hash, key, value, null)))
                break;                   // no lock when adding to empty bin
        }
        else if ((fh = f.hash) == MOVED)
            tab = helpTransfer(tab, f);
        else {
            V oldVal = null;
            synchronized (f) {
                if (tabAt(tab, i) == f) {
                    if (fh >= 0) {
                        binCount = 1;
                        for (Node<K,V> e = f;; ++binCount) {
                            K ek;
                            if (e.hash == hash &&
                                ((ek = e.key) == key ||
                                 (ek != null && key.equals(ek)))) {
                                oldVal = e.val;
                                if (!onlyIfAbsent)
                                    e.val = value;
                                break;
                            }
                            Node<K,V> pred = e;
                            if ((e = e.next) == null) {
                                pred.next = new Node<K,V>(hash, key,
                                                          value, null);
                                break;
                            }
                        }
                    }
                    else if (f instanceof TreeBin) {
                        Node<K,V> p;
                        binCount = 2;
                        if ((p = ((TreeBin<K,V>)f).putTreeVal(hash, key,
                                                       value)) != null) {
                            oldVal = p.val;
                            if (!onlyIfAbsent)
                                p.val = value;
                        }
                    }
                }
            }
            if (binCount != 0) {
                if (binCount >= TREEIFY_THRESHOLD)
                    treeifyBin(tab, i);
                if (oldVal != null)
                    return oldVal;
                break;
            }
        }
    }
    addCount(1L, binCount);
    return null;
}
```

- 1、首先判断底层用的数组是否为空长度是否为0
  
- 为空 则去初始化数组，执行initTable()方法
  
- 2、然后判断数组中当前位置是否为空
  
  - 是的话 直接将数据插入
  
- 3、判断容器是否在扩容（通过(fh = f.hash) == MOVED进行判断）
  
  - 在的话，当前线程去协助扩容
  
- 4、用syschronzied修饰代码块来保证多线程环境下插入操作的安全性
  - **首先将数组中的节点充当锁，同一时刻只有一个线程可以获取锁成功**
  - 判断是链表节点 还是树节点
    - 链表节点 ：遍历链表 判断 每一个节点的hash值和key值是否和要插入的节点的hash值和key值是否相同 ，相同则进行覆盖 不相同就接着遍历 ，都不相同的话就是用尾插法插入到队尾
    -  遍历红黑树，相同覆盖不相同就插入
  
- 5、判断是否需要树化

  首先判断链表长度是否>8

  - 是 则判断数组长度是否>64
    - 是 则树化
    - 否 则扩容

- 6、判断是否需要扩容

  在addConut()方法中计算容器中元素的个数，并且判断是否需要扩容【是否到达阈值】

  ```java
   addCount(1L, binCount);
  ```

  

##### 8.1 JDK1.8 中为什么使用内置锁 synchronized替换 可重入锁 ReentrantLock？

##### 8.2 concurrentHashMap 的put方法

- 1.7  : 首先根据计算出来的hash值 去找到对应的slot槽位，根据槽位找到对应的segment 。

  ​		然后尝试获取锁，如果锁已经被获取那么就自旋等待，超过一定的次数就阻塞的等待

  ​		获取锁成功之后就进行put操作

- 1.8：首先计算出key对应的hashCode，

##### 8.3 concurrentHashMap多线程扩容机制

（1）什么时候扩容

##### 8.4  concurrentHashMap和HashMap 和HashTable的区别

- concurrentHashMap和HashTable 

  【底层数据结构 ------------> 怎么实现线程安全】

- concurrentHashMap和HashMap 

  【是否是线程安全----------->使用的场景--------------是否可以存储key】

- HashMap 和HashTable

  【底层数据结构------->是否是线程安全---------->操作效率-------扩容机制------->是否可以存储nulls】



##### 8.5 JDK1.7 与 JDK1.8 中ConcurrentHashMap 的区别？










